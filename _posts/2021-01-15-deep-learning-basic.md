---
layout: post
title:  Deep Learning 기초
date:   2021-01-15
image:  .png
tags:   Data
---
## NN 기초
<BR/>

#### `퍼셉트론`
신경망을 이루는 가장 기본 단위로, 퍼셉트론은 다수의 신호(INPUT)를 입력받아서 하나의 신호(OUTPUT)를 출력한다.
<BR/><BR/>

#### `Weight`
INPUT으로 받은 값을 각 뉴런에 전달하게 되는데 그 과정에서 가중합을 활용한다. 즉, 가중치를 통해 활성화함수의 기울기를 변화시켜 결과 값에 대한 영향력의 크기를 결정한다.
<BR/><BR/>

#### `Bias`
가중합을 통해 각 뉴런으로 전달할 때 bias를 추가하여 연산하게 된다. bias를 통해 활성화함수를 왼쪽, 오른쪽으로 이동시켜 결과 값에 대한 영향력의 크기를 결정한다.
<BR/><BR/>

#### `활성화 함수`
* `sigmoid`<BR/>
sigmoid는 0과 1 사이의 범위를 가지고 있으며, 이진 분류 문제에 적절하다. 그림에서 보는 것처럼 INPUT 값이 매우 크거나 작을 경우 기울기가 0에 가까워진다. 이러한 경우, 층이 깊을수록 작은 기울기 값들이 곱해지면서 기울기가 점점 작아지는 gradient vanishing 문제가 발생할 수 있다. 또한, 학습을 지그재그 형태로 만들어 학습이 느려지는 문제가 발생한다.

* `tanh`<BR/>
sigmoid와 유사한 형태이지만 -1과 1 사이의 범위를 가지고 있다. 함수의 중심값을 0으로 옮겨 sigmoid의 학습 과정이 느려지는 문제를 해결했다. 하지만, 여전히 gradient vanishing 문제를 해결하지는 못했다.

* `ReLU`<BR/>
음수값은 0으로, 양수값은 그 값 그대로 출력하는 함수이다. 양수값이면 기울기가 항상 1이기 때문에 gradient vanishing 문제가 발생하지 않는다. 다만, 음수값이라면 기울기가 0이 되어 소멸한다는 문제가 발생한다.

* `Leaky ReLU`<BR/>
음수값을 가질 때 기울기가 0이 되어 소멸하는 ReLU의 문제점을 완화하기 위한 활성화 함수이다. 음수인 경우, 0.01이거나 다른 작은 값을 갖도록 한다.

* `softmax`<BR/>
softmax는 다중 분류에서 많이 사용되는 함수이다. 가장 높은 확률값을 가지는 클래스를 출력한다.
<BR/>

#### `Layer`
* `입력층 (Input layer)`<BR/>
데이터를 입력받는 층이다. 연산이 이루어지지 않으며, 신경망의 층 수에 포함되지 않는다.

* `은닉층 (Hidden layer)`<BR/>
입력층과 출력층 사이에 있는 층으로, 우리 눈에 직접적으로 보이지 않기 때문에 붙여진 이름이다.

* `출력층 (Output layer)`<BR/>
다른 층으로부터 받은 값을 활성화 함수를 통해 결과값으로 산출하는 층이다.<br><br><br><br>

## 신경망 학습
<BR/>

#### `손실함수`
실제 값과 출력 값의 차이를 최소화할 수 있는 가중치를 학습하게 되는데, 그 차이를 측정해주는 함수가 손실함수이다.




