---
layout: post
title:  Deep Learning 기초
date:   2021-01-15
image:  .png
tags:   Data
---
## NN 기초
<BR/>

#### `퍼셉트론`
신경망을 이루는 가장 기본 단위로, 퍼셉트론은 다수의 신호(INPUT)를 입력받아서 하나의 신호(OUTPUT)를 출력한다.
<BR/><BR/>

#### `Weight`
INPUT으로 받은 값을 각 뉴런에 전달하게 되는데 그 과정에서 가중합을 활용한다. 즉, 가중치를 통해 활성화함수의 기울기를 변화시켜 결과 값에 대한 영향력의 크기를 결정한다.
<BR/><BR/>

#### `Bias`
가중합을 통해 각 뉴런으로 전달할 때 bias를 추가하여 연산하게 된다. bias를 통해 활성화함수를 왼쪽, 오른쪽으로 이동시켜 결과 값에 대한 영향력의 크기를 결정한다.
<BR/><BR/>

#### `활성화 함수`
* `sigmoid`<BR/>
sigmoid는 0과 1 사이의 범위를 가지고 있으며, 이진 분류 문제에 적절하다. 그림에서 보는 것처럼 INPUT 값이 매우 크거나 작을 경우 기울기가 0에 가까워진다. 이러한 경우, 층이 깊을수록 작은 기울기 값들이 곱해지면서 기울기가 점점 작아지는 gradient vanishing 문제가 발생할 수 있다. 또한, 학습을 지그재그 형태로 만들어 학습이 느려지는 문제가 발생한다.

* `tanh`<BR/>
sigmoid와 유사한 형태이지만 -1과 1 사이의 범위를 가지고 있다. 함수의 중심값을 0으로 옮겨 sigmoid의 학습 과정이 느려지는 문제를 해결했다. 하지만, 여전히 gradient vanishing 문제를 해결하지는 못했다.

* `ReLU`<BR/>
음수값은 0으로, 양수값은 그 값 그대로 출력하는 함수이다. 양수값이면 기울기가 항상 1이기 때문에 gradient vanishing 문제가 발생하지 않는다. 다만, 음수값이라면 기울기가 0이 되어 소멸한다는 문제가 발생한다.

* `Leaky ReLU`<BR/>
음수값을 가질 때 기울기가 0이 되어 소멸하는 ReLU의 문제점을 완화하기 위한 활성화 함수이다. 음수인 경우, 0.01이거나 다른 작은 값을 갖도록 한다.

* `softmax`<BR/>
softmax는 다중 분류에서 많이 사용되는 함수이다. 가장 높은 확률값을 가지는 클래스를 출력한다.
<BR/>

#### `Layer`
* `입력층 (Input layer)`<BR/>
데이터를 입력받는 층이다. 연산이 이루어지지 않으며, 신경망의 층 수에 포함되지 않는다.

* `은닉층 (Hidden layer)`<BR/>
입력층과 출력층 사이에 있는 층으로, 우리 눈에 직접적으로 보이지 않기 때문에 붙여진 이름이다.

* `출력층 (Output layer)`<BR/>
다른 층으로부터 받은 값을 활성화 함수를 통해 결과값으로 산출하는 층이다.<br><br><br><br>

## 신경망 학습
<BR/>

#### `손실함수`
실제 값과 출력 값의 차이를 최소화할 수 있는 가중치를 학습하게 되는데, 그 차이를 측정해주는 함수가 손실함수이다.
* `MSE (Mean Squared Error)`<BR/>
회귀 문제를 풀고자 할 때 사용한다. 예측값과 실제 값의 차이에 대한 제곱을 평균으로 산출한 값이다.

* `MAE (Mean Absolute Error)`<BR/>
MSE와 마찬가지로 회귀 문제를 풀고자 할 때 사용하며, MSE와는 약간 다르게 제곱을 하지 않고 절대값에 대한 평균으로 산출한다.

* `Binary Crossentropy`<BR/>
이진 분류 문제를 풀고자 할 때 사용한다.

* `Categorial Crossentropy`<BR/>
3개 이상의 분류 문제를 풀고자 할 때 사용한다. 특히, 라벨이 [1, 0, 0], [0, 1, 0], [0, 0, 1]과 같이 one-hot 형태일 때 사용한다.

* `Sparse Categorical Crossentropy`<BR/>
3개 이상의 분류 문제를 풀고자 할 때 사용하며, 라벨이 정수 형태일 때 사용한다.
<BR/><BR/>

#### `Batch`
Batch는 전체 데이터 셋을 여러 작은 그룹으로 나누는 것을 말하며, 하나의 소그룹 안에 속하는 데이터의 크기를 batch size라고 한다. 것Batch size가 크면 메모리의 한계와 속도 저하 문제가 발생할 수 있기 때문에 적절한 batch size 할당이 필요하다.
<BR/><BR/>

#### `Epoch`
Epoch는 전체 데이터 셋을 몇 번 사용해서 학습을 할인가를 말한다. Epoch이 너무 작으면 Underfitting, 너무 크다면 Overfitting 문제가 발생할 수 있다.
<BR/><BR/>

#### `역전파`
역전파는 역방향으로 오차를 전파시키면서 각 층의 가중치를 업데이트하여 최적의 학습 결과를 찾아가는 과정이다. 순전파를 통해 출력층에서 계산된 오차에 따른 각 가중치의 미세 변화를 입력층 방향으로 역전파시키면서 가중치를 업데이트하고, 업데이트한 가중치를 활용하여 새로운 오차를 계산한 후, 이를 다시 역전파시켜 가중치를 업데이트하는 과정을 반복한다.
<BR/><BR/>

#### `학습 규제`
과적합을 막기 위한 방법
* `Early stopping`<BR/>
학습 과정에서 가장 적절한 지점의 가중치를 지나서 더 업데이트를 지속할 수도 있다. 따라서, 이를 방지하기 위해 early stopping을 사용함으로써 과하게 가중치가 업데이트 되는 것을 막는다.

* `Weight decay`<BR/>
weight decay는 가중치를 감소시키는 방법이다. 가중치가 과도하게 커지면 과적합이 발생할 수 있기 때문에 큰 가중치를 만들지 못하게 함으로써 과적합을 방지할 수 있다.

* `Dropout`<BR/>
dropout은 설정한 확률로 연결을 강제로 끊어주는 역할을 한다. 연결 없이 결과를 예측하도록 함으로써 과적합을 방지할 수 있다.

* `Contraint`<BR/>
가중치의 크기를 물리적으로 제한하는 방법이다. 설정한 값보다 가중치가 더 큰 경우 임의의 값으로 변경하는 방법이다.

* `Learning rate`<BR/>
학습이 진행될수록 학습률을 감소시키는 방법이다.
<BR/><BR/>

#### `Transfer learning`
기존 데이터로 학습된 네트워크를 재사용 가능하도록 하는 라이브러리이다. 학습 데이터를 적게 사용할 수 있고, 학습 속도가 빠르며, 더 잘 일반화된 모델을 만들 수 있다는 장점이 있다.<BR/><BR/><BR/><BR/>

## CNN
<BR/>

#### `Convolution`
* `filter`<BR/>
가중치의 집합으로 이루어져 데이터의 특징을 잡아낸다.

* stride : filter를 얼만큼씩 움직일 지 결정한다. Stride가 1이면 한 칸씩 움직이고, 2이면 두 칸씩 움직인다.
	* padding : 데이터 셋의 외곽에 0 또는 다른 값을 배치하는 방법이다. Padding을 하지 않을 경우, 데이터 셋의 각 모서리 부분은 다른 부분에 비해 사용되는 횟수가 줄어든다. 따라서 골고루 데이터를 이용하고자 사용하는 방법이다.

- Pooling : pooling은 convolution layer와 activation을 거쳐 나온 값을 중에서 일부를 추출하는 것이다. pooling에는 다양한 종류가 있지만, 대표적으로 가장 큰 값을 추출하는 max pooling, 평균값을 추출하는 average pooling이 있다.


